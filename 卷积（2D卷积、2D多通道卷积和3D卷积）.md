## 卷积（2D卷积、2D多通道卷积和3D卷积）

### 3D卷积Pytorch官方文档：https://pytorch.org/docs/master/generated/torch.nn.Conv3d.html?highlight=conv3d#torch.nn.Conv3d

### 2D卷积Pytorch官方文档：https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html?highlight=conv2d#torch.nn.Conv2d

### 1、2D卷积：

![img](https://pic3.zhimg.com/v2-8a6695c2e086525ac5a61610348739b2_b.webp)

二维卷积的过程，卷积核大小为3*3。

### 2、2D多通道卷积：

### 3、3D卷积：

![img](https://pic3.zhimg.com/v2-86e2bd970d07f9d6e1d921b248e45a3a_b.webp)

**Pytorch中conv3D的参数具体情况：**

torch.nn.Conv3d(*in_channels: int, out_channels: int, kernel_size: Union[T, Tuple[T, T, T]], stride: Union[T, Tuple[T, T, T]] = 1, padding: Union[T, Tuple[T, T, T]] = 0, dilation: Union[T, Tuple[T, T, T]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros'*)

Parameters

- **in_channels** ([*int*](https://docs.python.org/3/library/functions.html#int)) – Number of channels in the input image
- **out_channels** ([*int*](https://docs.python.org/3/library/functions.html#int)) – Number of channels produced by the convolution
- **kernel_size** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – Size of the convolving kernel
- **stride** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – Stride of the convolution. Default: 1
- **padding** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – Zero-padding added to all three sides of the input. Default: 0
- **padding_mode** (*string**,* *optional*) – `'zeros'`, `'reflect'`, `'replicate'` or `'circular'`. Default: `'zeros'`
- **dilation** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – Spacing between kernel elements. Default: 1
- **groups** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* *optional*) – Number of blocked connections from input channels to output channels. Default: 1
- **bias** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – If `True`, adds a learnable bias to the output. Default: `True`

![image-20201112150839787](C:\Users\WH\AppData\Roaming\Typora\typora-user-images\image-20201112150839787.png)





# [pytorch][基础模块] torch.nn.Conv3D 使用样例与说明

## **torch.nn.Conv3D**

3D卷积, 输入的shape是( N , C i n , D , H , W ) (N, C_{in},D,H,W)(*N*,*C**i**n*,*D*,*H*,*W*)，输出shape ( N , C o u t , D o u t , H o u t , W o u t ) (N,C_{out},D_{out},H_{out},W_{out})(*N*,*C**o**u**t*,*D**o**u**t*,*H**o**u**t*,*W**o**u**t*)

**实际使用建议:**
N N*N* 就是**batch_size**，无话可说；C i n C_{in}*C**i**n*​ 则对应着**输入图像的通道数**，如RGB\BGR图像这一维度就是3；D D*D*则是深度，如果是对于视频序列使用的3d conv，那么这个D D*D*实际对应的就是要执行卷积的**frame_size**，H , W H,W*H*,*W*对应的就是输入图像的**高和宽**

另外，使用的时候如果不指定具体size，如:`padding=2`，则是每一维度都为size是2的padding，也就是`padding=(2,2,2)`；如果指定每一维的padding的话则有就是指定的size，如:`padding=(3,2,1)`，则是分别是在三个维度上有3,2,1三种不同的padding，另外`kernel`和`stride`也是如此

**使用样例：**

```python3
import torch
import torch.nn as nn

# Sample intput | 随机输入
net_input = torch.randn(32, 3, 10, 224, 224)

# With square kernels and equal stride | 所有维度同一个参数配置
conv = nn.Conv3d(3, 64, kernel_size=3, stride=2, padding=1)
net_output = conv(net_input)
print(net_output.shape)  # shape=[32, 64, 5, 112, 112] | 相当于每一个维度上的卷积核大小都是3，步长都是2，pad都是1

# non-square kernels and unequal stride and with padding | 每一维度不同参数配置
conv = nn.Conv3d(3, 64, (2, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
net_output = conv(net_input)
print(net_output.shape) # shape=[32, 64, 9, 112, 112]
123456789101112131415
```

------

pytorch中3D的卷积使用还可以参考这个C3D的model：
[model传送门](https://github.com/jfzhang95/pytorch-video-recognition/blob/master/network/C3D_model.py)

pytorch官方文档：
[官方文档传送门](https://pytorch.org/docs/stable/nn.html#conv3d)



知乎介绍：作者：我不坏
链接：https://www.zhihu.com/question/266352189/answer/380438613
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。



更新：

- 2018-05-12：修改了kernel核的维度顺序，以更好的与一般的深度学习框架保持一致。

***************

最近要开始进入CT图像领域，故再思考了一遍3D卷积的思想，便在此挥毫，欲指点江山。

不过，首先还是要纠正题主的错误观念，即，在讨论卷积核的维度的时候，是不把channel维加进去的（或者说，卷积核的维度指的的进行滑窗操作的维度，而滑窗操作是不在channel维度上进行的，因为每个channel共享同一个滑窗位置, 但每个channel上的卷积核权重是独立的）。所以2D conv的卷积核其实是(c, k_h, k_w)，3D conv的卷积核就是(c, k_d, k_h, k_w)，其中k_d就是多出来的第三维，根据具体应用，在视频中就是时间维，在CT图像中就是层数维。

下面进入直观的2D conv和3D conv解释阶段。

------

1. 2D 卷积

2D卷积操作如图1所示，为了解释的更清楚，分别展示了单通道和多通道的操作。且为了画图方便，假定只有1个filter，即输出图像只有一个chanel。

其中，针对单通道，输入图像的channel为1，即输入大小为(1, height, weight)，卷积核尺寸为 (1, k_h, k_w)，卷积核在输入图像上的的空间维度（即(height, width)两维）上进行进行滑窗操作，每次滑窗和 (k_h, k_w) 窗口内的values进行卷积操作（现在都用相关操作取代），得到输出图像中的一个value。

针对多通道，假定输入图像的channel为3，即输入大小为(3, height, weight)，卷积核尺寸为 (3, k_h, k_w)， 卷积核在输入图像上的的空间维度（即(height, width)两维）上进行进行滑窗操作，每次滑窗与3个channels上的 (k_h, k_w) 窗口内的所有的values进行相关操作，得到输出图像中的一个value。

![img](https://pic4.zhimg.com/50/v2-61e8d9577583fd2b12a15a35006a5985_hd.jpg?source=1940ef5c)![img](https://pic4.zhimg.com/80/v2-61e8d9577583fd2b12a15a35006a5985_720w.jpg?source=1940ef5c)

\2. 3D 卷积

3D卷积操作如图2所示，同样分为单通道和多通道，且只使用一个filter，输出一个channel。

其中，针对单通道，与2D卷积不同之处在于，输入图像多了一个 depth 维度，故输入大小为(1, depth, height, width)，卷积核也多了一个k_d维度，因此卷积核在输入3D图像的空间维度（height和width维）和depth维度上均进行滑窗操作，每次滑窗与 (k_d, k_h, k_w) 窗口内的values进行相关操作，得到输出3D图像中的一个value.

针对多通道，输入大小为(3, depth, height, width)，则与2D卷积的操作一样，每次滑窗与3个channels上的 (k_d, k_h, k_w) 窗口内的所有values进行相关操作，得到输出3D图像中的一个value。

![img](https://pic4.zhimg.com/50/v2-30aec2d80aca0fadd2bd6a0282a1e2ca_hd.jpg?source=1940ef5c)

重点是：3D多通道卷积的直观理解

















##  4.11 一维到三维推广-深度学习第四课《卷积神经网络》-Stanford吴恩达教授

技术标签： [深度学习DL](https://www.pianshen.com/tag/深度学习DL/) [网络](https://www.pianshen.com/tag/网络/) [神经网络](https://www.pianshen.com/tag/神经网络/) [机器学习](https://www.pianshen.com/tag/机器学习/) [人工智能](https://www.pianshen.com/tag/人工智能/) [python](https://www.pianshen.com/tag/python/)

 

# 一维到三维推广 (1D and 3D Generalizations of Models)

你已经学习了许多关于卷积神经网络（**ConvNets**）的知识，从卷积神经网络框架，到如何使用它进行图像识别、对象检测、人脸识别与神经网络转换。即使我们大部分讨论的图像数据，某种意义上而言都是**2D**数据，考虑到图像如此普遍，许多你所掌握的思想不仅局限于**2D**图像，甚至可以延伸至**1D**，乃至**3D**数据。

![在这里插入图片描述](https://www.pianshen.com/images/19/f7a4dc107a029709df6107c824904b83.png)

让我们回头看看在第一周课程中你所学习关于**2D**卷积，你可能会输入一个14×14的图像，并使用一个5×5的过滤器进行卷积，接下来你看到了14×14图像是如何与5×5的过滤器进行卷积的，通过这个操作你会得到10×10的输出。

![在这里插入图片描述](https://www.pianshen.com/images/152/e643a5de00fdb83306e7d05357f01928.png)

如果你使用了多通道，比如14×14×3，那么相匹配的过滤器可能是5×5×3，如果你使用了多重过滤，比如16，最终你得到的是10×10×16。

![在这里插入图片描述](https://www.pianshen.com/images/990/b6e0af9c2c560fe62a69b5f9a721ae3e.png)

事实证明早期想法也同样可以用于1维数据，举个例子，左边是一个**EKG**信号，或者说是心电图，当你在你的胸部放置一个电极，电极透过胸部测量心跳带来的微弱电流，正因为心脏跳动，产生的微弱电波能被一组电极测量，这就是人心跳产生的**EKG**，每一个峰值都对应着一次心跳。

如果你想使用**EKG**信号，比如医学诊断，那么你将处理1维数据，因为**EKG**数据是由时间序列对应的每个瞬间的电压组成，这次不是一个14×14的尺寸输入，你可能只有一个14尺寸输入，在这种情况下你可能需要使用一个1维过滤进行卷积，你只需要一个1×5的过滤器，而不是一个5×5的。

![在这里插入图片描述](https://www.pianshen.com/images/201/499a4a9733cffa8788fbe163063f46a1.png)

二维数据的卷积是将同一个5×5特征检测器应用于图像中不同的位置（编号1所示），你最后会得到10×10的输出结果。1维过滤器可以取代你的5维过滤器（编号2所示），可在不同的位置中应用类似的方法（编号3，4，5所示）。

![在这里插入图片描述](https://www.pianshen.com/images/614/ff4d6a6b30bc22081eb7f3d84f5b8ff6.png)

当你对这个1维信号使用卷积，你将发现一个14维的数据与5维数据进行卷积，并产生一个10维输出。

![在这里插入图片描述](https://www.pianshen.com/images/72/9376e2339e2342ddbc834dbdbe46b400.png)

再一次如果你使用多通道，在这种场景下可能会获得一个14×1的通道。如果你使用一个**EKG**，就是5×1的，如果你有16个过滤器，可能你最后会获得一个10×16的数据，这可能会是你卷积网络中的某一层。

![在这里插入图片描述](https://www.pianshen.com/images/307/238c6771619ba9ce8ef844ffc332b023.png)

对于卷积网络的下一层，如果输入一个10×16数据，你也可以使用一个5维过滤器进行卷积，这需要16个通道进行匹配，如果你有32个过滤器，另一层的输出结果就是6×32，如果你使用了32个过滤器的话。

![在这里插入图片描述](https://www.pianshen.com/images/308/e832c325ee931b9f1ea3df8f1fbf1b14.png)

对于2D数据而言，当你处理10×10×16的数据时也是类似的，你可以使用5×5×16进行卷积，其中两个通道数16要相匹配，你将得到一个6×6的输出，如果你用的是32过滤器，输出结果就是6×6×32，这也是32的来源。

所有这些方法也可以应用于1维数据，你可以在不同的位置使用相同的特征检测器，比如说，为了区分**EKG**信号中的心跳的差异，你可以在不同的时间轴位置使用同样的特征来检测心跳。

所以卷积网络同样可以被用于**1D**数据，对于许多1维数据应用，你实际上会使用递归神经网络进行处理，这个网络你会在下一个课程中学到，但是有些人依旧愿意尝试使用卷积网络解决这些问题。

下一门课将讨论序列模型，包括递归神经网络、**LCM**与其他类似模型。我们将探讨使用**1D**卷积网络的优缺点，对比于其它专门为序列数据而精心设计的模型。

这也是**2D**向**1D**的进化，对于**3D**数据来说如何呢？什么是**3D**数据？与**1D**数列或数字矩阵不同，你现在有了一个**3D**块，一个**3D**输入数据。以你做**CT**扫描为例，这是一种使用X光照射，然后输出身体的**3D**模型，**CT**扫描实现的是它可以获取你身体不同片段（图片信息）。

![在这里插入图片描述](https://www.pianshen.com/images/790/8098b43bfc5a811b682a72502994663e.png)

当你进行**CT**扫描时，与我现在做的事情一样，你可以看到人体躯干的不同切片（整理者注：图中所示为人体躯干中不同层的切片，附**CT**扫描示意图，图片源于互联网），本质上这个数据是3维的。

![在这里插入图片描述](https://www.pianshen.com/images/147/008f069c0159e184f588099af8dda4a3.png)

一种对这份数据的理解方式是，假设你的数据现在具备一定长度、宽度与高度，其中每一个切片都与躯干的切片对应。

如果你想要在**3D**扫描或**CT**扫描中应用卷积网络进行特征识别，你也可以从第一张幻灯片（**Convolutions in 2D and 1D**）里得到想法，并将其应用到**3D**卷积中。为了简单起见，如果你有一个**3D**对象，比如说是14×14×14，这也是输入**CT**扫描的宽度与深度（后两个14）。再次提醒，正如图像不是必须以矩形呈现，**3D**对象也不是一定是一个完美立方体，所以长和宽可以不一样，同样**CT**扫描结果的长宽高也可以是不一致的。为了简化讨论，我仅使用14×14×14为例。

![在这里插入图片描述](https://www.pianshen.com/images/991/04670e8ee21ead50cadf8afcb63534a7.png)

如果你现在使用5×5×5过滤器进行卷积，你的过滤器现在也是**3D**的，这将会给你一个10×10×10的结果输出，技术上来说你也可以再×1（编号1所示），如果这有一个1的通道。这仅仅是一个**3D**模块，但是你的数据可以有不同数目的通道，那种情况下也是乘1（编号2所示），因为通道的数目必须与过滤器匹配。如果你使用16过滤器处理5×5×5×1，接下来的输出将是10×10×10×16，这将成为你**3D**数据卷积网络上的一层。

![在这里插入图片描述](https://www.pianshen.com/images/264/653dc0714e27837d4bf6aece3ea27cf0.png)

如果下一层卷积使用5×5×5×16维度的过滤器再次卷积，通道数目也与往常一样匹配，如果你有32个过滤器，操作也与之前相同，最终你得到一个6×6×6×32的输出。

某种程度上**3D**数据也可以使用**3D**卷积网络学习，这些过滤器实现的功能正是通过你的**3D**数据进行特征检测。**CT**医疗扫描是**3D**数据的一个实例，另一个数据处理的例子是你可以将电影中随时间变化的不同视频切片看作是**3D**数据，你可以将这个技术用于检测动作及人物行为。

总而言之这就是**1D**、**2D**及**3D**数据处理，图像数据无处不在，以至于大多数卷积网络都是基于图像上的**2D**数据，但我希望其他模型同样会对你有帮助。

这是本周最后一次视频，也是最后一次关于卷积神经网络的课程，你已经学习了许多关于卷积网络的知识，我希望你能够在未来工作中发现许多思想对你有所裨益，祝贺你完成了这些视频学习，我希望你能喜欢这周的课后练习，接下来关于顺序模型的课程我们不见不散。

参考文献：

- Florian Schroff, Dmitry Kalenichenko, James Philbin (2015). [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/pdf/1503.03832.pdf)
- Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, Lior Wolf (2014). [DeepFace: Closing the gap to human-level performance in face verification](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf)
- The pretrained model we use is inspired by Victor Sy Wang’s implementation and was loaded using his code: https://github.com/iwantooxxoox/Keras-OpenFace
  .
- Our implementation also took a lot of inspiration from the official FaceNet github repository: https://github.com/davidsandberg/facenet
- Leon A. Gatys, Alexander S. Ecker, Matthias Bethge, (2015). A Neural Algorithm of Artistic Style (https://arxiv.org/abs/1508.06576
  )
- Harish Narayanan, Convolutional neural networks for artistic style transfer. https://harishnarayanan.org/writing/artistic-style-transfer/
- Log0, TensorFlow Implementation of “A Neural Algorithm of Artistic Style”. http://www.chioka.in/tensorflow-implementation-neural-algorithm-of-artistic-style
- Karen Simonyan and Andrew Zisserman (2015). Very deep convolutional networks for large-scale image recognition (https://arxiv.org/pdf/1409.1556.pdf
  )
- MatConvNet. http://www.vlfeat.org/matconvnet/pretrained/

## 课程板书

![在这里插入图片描述](https://www.pianshen.com/images/832/0da1171884192e8b17f5894938c097c0.png)
![在这里插入图片描述](https://www.pianshen.com/images/457/342022748ac8f72d3aef1a7ece0d0d11.png)
![在这里插入图片描述](https://www.pianshen.com/images/68/9c49f49a5b029d403aa80198b5bedf7c.png)
![在这里插入图片描述](https://www.pianshen.com/images/690/3f31fe3d7e9c1097a79b7701383d5622.png)

版权声明：本文为博主原创文章，遵循[ CC 4.0 BY-SA ](https://creativecommons.org/licenses/by-sa/4.0/)版权协议，转载请附上原文出处链接和本声明。本文链接：https://blog.csdn.net/weixin_36815313/article/details/105814954











## 1.三维立体图像（RGB三通道）的卷积运算 

![img](https://img-blog.csdn.net/2018030216305926?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA5NzkwMTc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

6$\times$6$\times$3分别代表RGB图像的 高、宽、通道数；3$\times$3$\times$3分别代表滤波器矩阵 高、宽、通道数。**图像和滤波器通道数必须相等（匹配）**，通道数与输入图像的通道数需要是相同的，而且卷积核中也有对应于RGB分为三个通道，每个通道上可以设置权重，以不同的权重来提取RGB三种信息，这样通过分配不同的权重，就可以实现对不同信息的提取。

将3*3*3滤波器转换成立方体，一共3$\times$3=27个数值，（按照前面章节介绍的卷积运算计算）。分别乘与滤波器对应的RGB图像三个通道的数值，再相加得到4*4输出矩阵的值。

## 2.3$\times$3$\times$3滤波器作用

![img](https://img-blog.csdn.net/20180302164025277?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA5NzkwMTc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

检测某一通道的边缘特征：将3$\times$3$\times$3滤波器矩阵的R通道设为垂直边缘检测矩阵，G B通道设为0，即可检测R通道的垂直边缘。（滤波器只关注一个通道是可行的）

不区分通道检查边缘特征：将RGB三个通道都设为垂直边缘检测，可以不区分通道检查垂直边缘情况。

## 3.如何同时检测到垂直水平或者任意角度的边缘？

![img](https://img-blog.csdn.net/2018030217100582?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA5NzkwMTc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)



将图像矩阵分别与2个检测不同角度边缘的滤波器矩阵卷积，得到2个4$\times$4输出矩阵，输出结果即对应不同边缘检测结果。（将2个4$\times$4矩阵放在一起组成立方体矩阵）

## 4.总结 

![img](https://img-blog.csdn.net/20180302172140176?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA5NzkwMTc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

nc是通道数（文献中常叫通道数或者深度，通道数较好理解），图像和滤波器矩阵通道数必须相等，为检测多个边缘特征，可以设置多个滤波器，滤波器数量为nc’。（这里不考虑padding和strided情况）

卷积得到矩阵的第三个维度=nc'。

![tf.nn.conv3d应该这样用](https://pic2.zhimg.com/v2-15af14e2fd7b55663ccab10a29b2863a_1440w.jpg?source=172ae18b)

# tf.nn.conv3d应该这样用

[![空字符](https://pic1.zhimg.com/v2-15fc5d225ca6754db38c4bb3b69c1e36_xs.jpg?source=172ae18b)](https://www.zhihu.com/people/the_lastest)

[空字符](https://www.zhihu.com/people/the_lastest)

关注公众号：‘月来客栈’，带你看不一样的江湖！

关注他

16 人赞同了该文章

### **1. 视觉角度**

我们首先先通过一张图来直观的看看2D与3D卷积的区别：



![img](https://pic2.zhimg.com/80/v2-b8a750c91ad3bb5e9c7ba114e0c21d21_720w.jpg)




从图p0116中（只包含一个卷积核）我们可以看出，对于：

- 2D convolution: 使用场景一般是单通道的数据（例如MNIST），输出也是单通道，对整个通道同时执行卷积操作；

- 2D convolution on multiple frames: 使用场景一般是多通道的数据（例如cifar-10），输出也是单通道，对整个通道同时执行卷积操作；

- - **2D卷积在执行时是在各自的通道中共享卷积核；**

- **3D convolution:** 使用场景一般是多帧（单/多通道）的frame-like数据（视频帧），且输出也是多帧，依次对连续k帧的整个通道同时执行卷积操作；

- - **3D卷积在执行时不仅在各自的通道中共享卷积核，而且在各帧（连续k帧）之间也共享卷积核；**
    ​

![img](https://pic3.zhimg.com/80/v2-7c3692f3777d84f88f18877c280da8de_720w.jpg)

**2. 计算角度**

话说数无形时少直觉，形少数时难入微。在我们从视觉角度观察之后，我们再来从计算的角度看看3D卷积到底是怎么在工作。

假设现在有一个3帧的画面，且每一帧有2个通道，在时间维度的跨度为2帧，卷积核的宽度为3

- 我们首先再次从视觉的角度看看这个结果：

![img](https://pic1.zhimg.com/80/v2-b2e7adb9f0025e7ca4388cbffad84b14_720w.jpg)

- 计算结果：
  由于在时间维度的跨度为2帧，且每帧有2个通道，所以从“矩阵”个数来看的话，我们的卷积核应该有4矩阵。
  ​

![img](https://pic4.zhimg.com/80/v2-081c50ef57b7fd84db42ed9fd1305b07_720w.jpg)

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+321%26%3D1%2B6%2B9%2B18%2B23%2B33%2B34%2B42%2B50%2B51%2B54%5C%5C%5B2ex%5D+728%26%3D38%2B43%2B46%2B55%2B60%2B70%2B71%2B79%2B87%2B88%2B91+%5Cend%7Baligned%7D)

### **3. tensorflow示例**

### **3.1 接口介绍**

在知道3D CNN的原理之后，我们现在来看怎么用tensorflow提供的接口来实现上面的计算操作。首先根据上面的示例，我们有了下列参数：

- 输入数据：

- - batch = 1;
  - in_depth = 3; 序列长度
  - in_channels = 2; 每一帧的通道数
  - in_height = 4;
  - in_width = 4;



- 卷积核：

- - filter_depth = 2; 时间维度的连续跨度
  - filter_height = 3;
  - filter_width = 3;
  - in_channels = 2; 输入时每帧的通道，必须核输入数据的通道一样
  - out_channels = 1;卷积核的个数，对应的就是输出之后每帧的通道数



```python
  def conv3d(input, filter, strides, padding):
```

这是`conv3d`的接口，其主要接收4个参数：

```
input` : 输入，其格式为`[batch, in_depth, in_height, in_width, in_channels].
filter`: 卷积核，其格式为`[filter_depth, filter_height, filter_width, in_channels,out_channels]
```

`strides`: 移动步长`[1,1,1,1,1]`即可

`padding`: 是否padding

最后，其输出结果的格式同输入，也为`[batch, in_depth, in_height, in_width, in_channels].`

### **3.2 生成数据和实现**

在介绍完接口后，只需要给定数据即可了。为了验证第二节中实验，我们下面先生成数据，然后再进行卷积。

- 生成数据：

- - 输入

```text
 image_in_man = np.linspace(1, 96, 96).reshape(1, 3, 2, 4, 4)  
 # [batch, in_depth, in_channels, in_height, in_width]
 
 image_in_tf = image_in_man.transpose(0, 1, 3, 4, 2)  
 # [batch, in_depth, in_height, in_width, in_channels].
```


值得注意的是，为了查看我们生成的数据，我们将`inchannels`这个维度放在了第2个（从0开始）维度，因为这样看才直观（详见[tf.nn.conv2d 你真的会用么](https://link.zhihu.com/?target=https%3A//blog.csdn.net/The_lastest/article/details/85269027))，但在喂给`conv3d`是要转成其接收的格式

- - 卷积核

```python3
 weight_in_man = np.array(
     [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
  1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0]).reshape(1, 2, 2, 3, 3) 
 # [out_channels,filter_depth, in_channels,filter_height, filter_width, ]
 
 weight_in_tf = weight_in_man.transpose(1, 3, 4, 2,0)  
 # [filter_depth, filter_height, filter_width, in_channels,out_channels]
```

- 计算

```python3
 import tensorflow as tf
 import numpy as np
 
 image_in_man = np.linspace(1, 96, 96).reshape(1, 3, 2, 4, 4)
 # [batch, in_depth, in_channels, in_height, in_width]
 image_in_tf = image_in_man.transpose(0, 1, 3, 4, 2)
 # [batch, in_depth, in_height, in_width, in_channels].
 # shape:[1,2,4,4,2]
 weight_in_man = np.array(
     [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
  1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0]).reshape(1, 2, 2, 3, 3)  # 1,3,4,2,0
 weight_in_tf = weight_in_man.transpose(1, 3, 4, 2, 0)
 # [filter_depth, filter_height, filter_width, in_channels,out_channels]
 # shape: [2,3,3,2,1]
 print(image_in_man)
 print(weight_in_man)
 
 x = tf.placeholder(dtype=tf.float32, shape=[1, 3, 4, 4, 2], name='x')
 w = tf.placeholder(dtype=tf.float32, shape=[2, 3, 3, 2, 1], name='w')
 conv = tf.nn.conv3d(x, w, strides=[1, 1, 1, 1, 1], padding='VALID')
 with tf.Session() as sess:
  r_in_tf = sess.run(conv, feed_dict={x: image_in_tf, w: weight_in_tf})
  # [batch, in_depth, in_height, in_width, in_channels].
  print(r_in_tf.shape)
  r_in_man = r_in_tf.transpose(0, 1, 4, 2, 3)
  # [batch, in_depth,in_channels,in_height, in_width].
  print(r_in_man)
 
```

- 结果

```text
 [[[[[ 1.  2.  3.  4.]
     [ 5.  6.  7.  8.]
     [ 9. 10. 11. 12.]
     [13. 14. 15. 16.]]
 
    [[17. 18. 19. 20.]
     [21. 22. 23. 24.]
     [25. 26. 27. 28.]
     [29. 30. 31. 32.]]]
 
 
   [[[33. 34. 35. 36.]
     [37. 38. 39. 40.]
     [41. 42. 43. 44.]
     [45. 46. 47. 48.]]
 
    [[49. 50. 51. 52.]
     [53. 54. 55. 56.]
     [57. 58. 59. 60.]
     [61. 62. 63. 64.]]]
 
 
   [[[65. 66. 67. 68.]
     [69. 70. 71. 72.]
     [73. 74. 75. 76.]
     [77. 78. 79. 80.]]
 
    [[81. 82. 83. 84.]
     [85. 86. 87. 88.]
     [89. 90. 91. 92.]
     [93. 94. 95. 96.]]]]]
 [[[[[1 0 0]
     [0 1 0]
     [1 0 0]]
 
    [[0 1 0]
     [0 0 1]
     [0 0 0]]]
 
 
   [[[1 1 0]
     [0 0 0]
     [0 1 0]]
 
    [[0 1 1]
     [0 1 0]
     [0 0 0]]]]]
  
  
    (1, 2, 2, 2, 1)
 [[[[[321. 332.]
     [365. 376.]]]
 
 
   [[[673. 684.]
     [717. 728.]]]]]
```







# 三维卷积详解

![img](https://csdnimg.cn/release/blogv2/dist/pc/img/original.png)

[you是mine](https://me.csdn.net/qq_28949847) 2020-06-30 17:13:28 ![img](https://csdnimg.cn/release/blogv2/dist/pc/img/articleReadEyes.png) 416 ![img](https://csdnimg.cn/release/blogv2/dist/pc/img/tobarCollect.png) 收藏 1

分类专栏： [人工智能](https://blog.csdn.net/qq_28949847/category_9592662.html) 文章标签： [过滤器](https://www.csdn.net/gather_27/MtTaEg0sNDU1OTMtYmxvZwO0O0OO0O0O.html) [卷积](https://www.csdn.net/gather_29/MtTaEg0sMzY2MDMtYmxvZwO0O0OO0O0O.html) [计算机视觉](https://www.csdn.net/gather_23/MtTaggwsNTg2MC1ibG9n.html) [卷积神经网络](https://www.csdn.net/gather_24/NtTaIg3sMDQ3Ny1ibG9n.html) [三维卷积](https://so.csdn.net/so/search/s.do?q=三维卷积&t=blog&o=vip&s=&l=&f=&viparticle=)

版权

假如说你不仅想检测灰度图像的特征，也想检测 RGB 彩色图像的特征。彩色图像如果是 6×6×3，这里的 3指的是三个颜色通道，你可以把它想象成三个 6×6图像的堆叠。为了检测图像的边缘或者其他的特征，不是把它跟原来的 3×3 的过滤器做卷积，而是跟一个三维的过滤器，它的维度是 3×3×3，这样这个过滤器也有三层，对应红绿、蓝三个通道。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200630170935828.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI4OTQ5ODQ3,size_16,color_FFFFFF,t_70)

给这些起个名字（原图像），这里的第一个 6 代表图像高度，第二个 6 代表宽度，这个3 代表通道的数目。同样你的过滤器也有高，宽和通道数，并且图像的通道数必须和过滤器的通道数匹配，所以这两个数（紫色方框标记的两个数）必须相等。

这个卷积操作会是一个 4×4 的图像，注意是 4×4×1，最后一个数不是 3 了。

给这些起个名字（原图像），这里的第一个 6 代表图像高度，第二个 6 代表宽度，这个3 代表通道的数目。同样你的过滤器也有高，宽和通道数，并且**图像的通道数必须和过滤器的通道数匹配**，所以这两个数（紫色方框标记的两个数）必须相等。

这个卷积操作会是一个 4×4 的图像，注意是 4×4×1，最后一个数不是 3 了。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200630170959344.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI4OTQ5ODQ3,size_16,color_FFFFFF,t_70)
这个是 6×6×3 的图像，这个是3×3×3 的过滤器，最后一个数字通道数必须和过滤器中的通道数相匹配。为了简化这个 3×3×3过滤器的图像，我们不把它画成 3 个矩阵的堆叠，而画成这样，一个三维的立方体。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200630171021568.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI4OTQ5ODQ3,size_16,color_FFFFFF,t_70)
为了计算这个卷积操作的输出，你要做的就是把这个 3×3×3 的过滤器先放到最左上角的位置，这个 3×3×3 的过滤器有 27 个数， 27 个参数就是 3 的立方。依次取这 27 个数，然后乘以相应的红绿蓝通道中的数字。先取红色通道的前 9 个数字，然后是绿色通道，然后再是蓝色通道，乘以左边黄色立方体覆盖的对应的 27 个数，然后把这些数都加起来，就得到了输出的第一个数字。

如果要计算下一个输出，你把这个立方体滑动一个单位，再与这 27 个数相乘，把它们都加起来，就得到了下一个输出，以此类推。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200630171048940.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI4OTQ5ODQ3,size_16,color_FFFFFF,t_70)
那么，这个能干什么呢？举个例子，这个过滤器是 3×3×3 的，如果你想检测图像红色通道的边缘，那么你可以将第一个过滤器设为：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200630171112117.png)
而绿色通道全为 0：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200630171128951.png)
蓝色也全为 0。如果你把这三个堆叠在一起形成一个 3×3×3 的过滤器，那么这就是一个检测垂直边界的过滤器，但只对红色通道有用。或者如果你不关心垂直边界在哪个颜色通道里，那么你可以用一个这样的过滤器：

所有三个通道都是这样。所以通过设置第二个过滤器参数，你就有了一个边界检测器， 3×3×3 的边界检测器，用来检测任意颜色通道里的边界。参数的选择不同，你就可以得到不同的特征检测器，所有的都是 3×3×3 的过滤器。

按照计算机视觉的惯例，当你的输入有特定的高宽和通道数时， 你的过滤器可以有不同的高，不同的宽，但是必须一样的通道数。理论上，我们的过滤器只关注红色通道，或者只关注绿色或者蓝色通道也是可行的。

再注意一下这个卷积立方体，一个 6×6×6 的输入图像卷积上一个 3×3×3 的过滤器，得到一个 4×4 的二维输出。

如果你想同时用多个过滤器怎么办？

这个 6×6×3 的图像和这个 3×3×3 的过滤器卷积，得到 4×4 的输出。（第一个）这可能是一个垂直边界检测器或者是学习检测其他的特征。第二个过滤器可以用橘色来表示，它可以是一个水平边缘检测器。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200630171151532.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI4OTQ5ODQ3,size_16,color_FFFFFF,t_70)
所以和第一个过滤器卷积，可以得到第一个 4×4 的输出，然后卷积第二个过滤器，得到一个不同的 4×4 的输出。我们做完卷积，然后把这两个 4×4 的输出，取第一个把它放到前面，然后取第二个过滤器输出，所以把这两个输出堆叠在一起，这样你就都得到了一个 4×4×2 的输出立方体。它用 6×6×3 的图像，然后卷积上这两个不同的 3×3 的过滤器，得到两个 4×4 的输出，它们堆叠在一起，形成一个 4×4×2 的立方体，这里的 2 的来源于我们用了两个不同的过滤器。

如果你有一个n∗n∗nc（通道数）的输入图像，在这个例子中就是 6×6×3，这里的nc就是通道数目，然后卷积上一个f∗f∗nc，这个例子中是 3×3×3，然后你就得到了(n - f + 1) \times (n - f + 1) \times {n_{{c^’}}}:
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200630171224387.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI4OTQ5ODQ3,size_16,color_FFFFFF,t_70)
这里{n_{{c^’}}}其实就是下一层的通道数，它就是你用的过滤器的个数，在我们的例子中，那就是 4×4×2。这个假设时用的步幅为 1，并且没有 padding。如果你用了不同的步幅或者 padding，那么这个n − f + 1数值会变化。

这个对立方体卷积的概念真的很有用，你现在可以用它的一小部分直接在三个通道的RGB 图像上进行操作。更重要的是，你可以检测两个特征，比如垂直和水平边缘或者 10 个或者 128 个或者几百个不同的特征，并且输出的通道数会等于你要检测的特征数。

# 深度学习之3D卷积神经网络

![img](https://csdnimg.cn/release/blogv2/dist/pc/img/original.png)

[zlinxi](https://me.csdn.net/qq_33273962) 2018-11-04 10:34:11 ![img](https://csdnimg.cn/release/blogv2/dist/pc/img/articleReadEyes.png) 17677 ![img](https://csdnimg.cn/release/blogv2/dist/pc/img/tobarCollect.png) 收藏 73

分类专栏： [机器学习](https://blog.csdn.net/qq_33273962/category_8281175.html) [深度学习](https://blog.csdn.net/qq_33273962/category_8281176.html) 文章标签： [3D卷积神经网络](https://www.csdn.net/gather_22/NtTaIg0sMTIwOS1ibG9n.html) [深度学习](https://www.csdn.net/gather_2d/MtTaggxsNzMxNC1ibG9n.html)

版权

一、概述

3D CNN主要运用在视频分类、动作识别等领域，它是在2D CNN的基础上改变而来。由于2D CNN不能很好的捕获时序上的信息，因此我们采用3D CNN，这样就能将视频中时序信息进行很好的利用。首先我们介绍一下2D CNN与3D CNN的区别。如图1所示，a)和b)分别为2D卷积用于单通道图像和多通道图像的情况（此处多通道图像可以指同一张图片的3个颜色通道，也指多张堆叠在一起的图片，即一小段视频），对于一个滤波器，输出为一张二维的特征图，多通道的信息被完全压缩了。而c)中的3D卷积的输出仍然为3D的特征图。也就是说采用2D CNN对视频进行操作的方式，一般都是对视频的每一帧图像分别利用CNN来进行识别，这种方式的识别没有考虑到时间维度的帧间运动信息，而使用3D CNN能更好的捕获视频中的时间和空间的特征信息。

![img](https://img-blog.csdnimg.cn/2018110409395176.png)

​                                             图一 

 二、原理

首先我们看一下3D CNN是如何对时间维度进行操作的，如图二所示，我们将时间维度看成是第三维，这里是对连续的四帧图像进行卷积操作，3D卷积是通过堆叠多个连续的帧组成一个立方体，然后在立方体中运用3D卷积核。在这个结构中，卷积层中每一个特征map都会与上一层中多个邻近的连续帧相连，因此捕捉运动信息。

![img](https://img-blog.csdnimg.cn/20181104094535571.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjczOTYy,size_16,color_FFFFFF,t_70)

​                                                  图二 

注：3D卷积核只能从cube中提取一种类型的特征，因为在整个cube中卷积核的权值都是一样的，也就是共享权值，都是同一个卷积核（图中同一个颜色的连接线表示相同的权值）。我们可以采用多种卷积核，以提取多种特征 。

图三是3D卷积神经网络的整体架构

![img](https://img-blog.csdnimg.cn/20181104100334956.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjczOTYy,size_16,color_FFFFFF,t_70)

​                                                  图三 

**input—>H1**

   神经网络的输入为7张大小为60$\times$40的连续帧，7张帧通过事先设定硬核（hardwired kernels）获得5种不同特征：灰度、x方向梯度、y方向梯度、x方向光流、y方向光流，前面三个通道的信息可以直接对每帧分别操作获取，后面的光流（x，y）则需要利用两帧的信息才能提取，因此H1层的特征maps数量：（7+7+7+6+6=33），特征maps的大小依然是60$\times$ 40。

**H1—>C2**

   用两个7*7*3的3D卷积核对5个channels分别进行卷积，获得两个系列，每个系列5个channels（7* 7表示空间维度，3表示时间维度，也就是每次操作3帧图像），同时，为了增加特征maps的个数，在这一层采用了两种不同的3D卷积核，因此C2层的特征maps数量为：（（（7-3）+1）此处的（7-3）+1是一种计算方法，每次提取三帧一共七帧，最后提取出的信息的次数也就是个数，就按照此种方式计算。3+（（6-3）+1）$\times$ 2）$\times$ 2=23$\times$ 2。这里右乘的2表示两种卷积核。特征maps的大小为：（（60-7）+1）* （（40-7）+1）=54 * 34。然后为卷积结果加上偏置套一个tanh函数进行输出。（典型神经网。）

![img](https://images0.cnblogs.com/blog/519499/201311/29200229-4233d68d65bd40fc93352212fccb8adb.png)

 **C2—>S3**

  2x2池化，下采样。下采样之后的特征maps数量保持不变，因此S3层的特征maps数量为：23 $\times$2。特征maps的大小为：（（54 / 2）$\times$（34 /2）=27 $\times$17

**S3—>C4**

  为了提取更多的图像特征，用**三个**7$\times$6$\times$3的3D卷积核分别对各个系列各个channels进行卷积，获得**6个系列**，每个系列依旧5个channels的大量maps。

我们知道，从输入的7帧图像获得了5个通道的信息，因此结合总图S3的上面一组特征maps的数量为（（7-3）+1）$\times$ 3+（（6-3）+1）$\times$2=23,可以获得各个通道在S3层的数量分布：

前面的乘3表示gray通道特征maps数量= gradient-x通道maps数量= gradient-y通道maps数量=（7-3）+1）=5；

后面的乘2表示optflow-x通道特征maps数量=optflow-y通道maps数量=（6-3）+1=4；

假设对总图S3的上面一组特征maps采用一种7 6 3的3D卷积核进行卷积就可以获得：

（（5-3）+1）$\times$ 3+（（4-3）+1）$\times$2=9+4=13；

三种不同的3D卷积核就可获得13$\times$3个特征maps，同理对总图S3的下面一组特征maps采用三种不同的卷积核进行卷积操作也可以获得13*3个特征maps，

因此C4层的特征maps数量：13$\times$3$\times$2=13$\times$6

C4层的特征maps的大小为：（（27-7）+1）$\times$（（17-6）+1）=21$\times$12

然后加偏置套tanh。

**C4—>S5**

3X3池化，下采样。此时每个maps的大小：7$\times$ 4。通道maps数量分布情况如下：

gray通道maps数量= gradient-x通道maps数量= gradient-y通道maps数量=3

optflow-x通道maps数量=optflow-y通道maps数量=2；

**S5—>C6**

   进行了两次3D卷积之后，时间上的维数已经被压缩得无法再次进行3D卷积（两个光流channels只有两个maps）。此时对各个maps用7*42D卷积核进行卷积，加偏置套tanh，获得C6层。C6层维度已经相当小，flatten为一列有128个节点的神经网络层。

**C6—>output**

   经典神经网络模型两层之间全链接，output的节点数目随标签而定。

三、应用

这里可以将3D卷积神经网络应用在卷积自编码器上，最近看了一篇3D CAE在高光谱图像上应用的文章：LEARNING SENSOR-SPECIFIC FEATURES FOR HYPERSPECTRAL IMAGES VIA 3-DIMENSIONAL CONVOLUTIONAL AUTOENCODER。这篇文章将高光谱图像的波段这一维转化成3D卷积神经网络的时间维，在卷积自编码器中使用3D卷积，得到了很好的效果，下面是其中卷积核和通道的变化表格。

![img](https://img-blog.csdnimg.cn/20181104103300432.png)

![img](https://img-blog.csdnimg.cn/20181104103310820.png)

 

参考：

https://blog.csdn.net/qq_25737169/article/details/75072014 

https://blog.csdn.net/AUTO1993/article/details/70948249

https://www.cnblogs.com/Ponys/p/3450177.html



































# CNN中feature map、卷积核、卷积核个数、filter、channel的概念解释，以及CNN 学习过程中卷积核更新的理解

![img](https://csdnimg.cn/release/blogv2/dist/pc/img/original.png)

[xys430381_1](https://me.csdn.net/xys430381_1) 2018-09-08 15:22:40 ![img](https://csdnimg.cn/release/blogv2/dist/pc/img/articleReadEyes.png) 64631 ![img](https://csdnimg.cn/release/blogv2/dist/pc/img/tobarCollect.png) 收藏 463

分类专栏： [图像处理](https://blog.csdn.net/xys430381_1/category_8027337.html) [深度学习](https://blog.csdn.net/xys430381_1/category_7165154.html) 文章标签： [cnn](https://so.csdn.net/so/search/s.do?q=cnn&t=blog&o=vip&s=&l=&f=&viparticle=)

版权

# feature map、卷积核、卷积核个数、filter、channel的概念解释

## feather map的理解

在cnn的每个卷积层，数据都是以三维形式存在的。你可以把它看成许多个二维图片叠在一起（像豆腐皮一样），其中每一个称为一个**feature map**。

### feather map 是怎么生成的？

**输入层：**在输入层，如果是灰度图片，那就只有一个feature map；如果是彩色图片，一般就是3个feature map（红绿蓝）。

**其它层：**层与层之间会有若干个卷积核（kernel）（也称为过滤器），上一层每个feature map跟每个卷积核做卷积，都会产生下一层的一个feature map，有N个卷积核，下层就会产生N个feather map。

### 多个feather map的作用是什么？

在卷积神经网络中，我们希望用一个网络模拟视觉通路的特性，分层的概念是自底向上构造简单到复杂的神经元。楼主关心的是同一层，那就说说同一层。
我们希望构造一组基，这组基能够形成对于一个事物完备的描述，例如描述一个人时我们通过描述身高/体重/相貌等，在卷积网中也是如此。在同一层，我们希望得到对于一张图片多种角度的描述，具体来讲就是用多种不同的卷积核对图像进行卷，得到不同核（这里的核可以理解为描述）上的响应，作为图像的特征。**他们的联系在于形成图像在同一层次不同基上的描述。**

下层的核主要是一些简单的边缘检测器（也可以理解为生理学上的simple cell）。

上层的核主要是一些简单核的叠加（或者用其他词更贴切），可以理解为complex cell。

 

多少个Feature Map？真的不好说，简单问题少，复杂问题多，但是自底向上一般是核的数量在逐渐变多（当然也有例外，如Alexnet），主要靠经验。

## 卷积核的理解

***\*卷积核\****在有的文档里也称为**过滤器（filter）：**  

- 每个卷积核具有长宽深三个维度；
- 在某个卷积层中，可以有多个卷积核：下一层需要多少个feather map，本层就需要多少个卷积核。

### 卷积核的形状

每个卷积核具有长、宽、深三个维度。在CNN的一个卷积层中：

- 卷积核的长、宽都是人为指定的，长X宽也被称为卷积核的尺寸，常用的尺寸为3X3，5X5等；
- 卷积核的深度与当前图像的深度（feather map的张数）相同，所以指定卷积核时，只需指定其长和宽 两个参数。例如，在原始图像层 （输入层），如果图像是灰度图像，其feather map数量为1，则卷积核的深度也就是1；如果图像是rgb图像，其feather map数量为3，则卷积核的深度也就是3.

### 卷积核个数的理解

如下图红线所示：该层卷积核的个数，有多少个卷积核，经过卷积就会产生多少个feature map，也就是下图中 `豆腐皮儿`的层数、同时也是下图`豆腐块`的深度（宽度）！！这个宽度可以手动指定，一般网络越深的地方这个值越大，因为随着网络的加深，feature map的长宽尺寸缩小，本卷积层的每个map提取的特征越具有代表性（精华部分），所以后一层卷积层需要增加feature map的数量，才能更充分的提取出前一层的特征，一般是成倍增加（不过具体论文会根据实验情况具体设置）！

![img](https://img-blog.csdn.net/20180605134623321?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MjMxNTQ5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

 

![img](https://img-blog.csdn.net/20180605134717740?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MjMxNTQ5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

 

 

![img](https://img-blog.csdn.net/20180605135241270?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MjMxNTQ5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

##  

## 卷积核的运算过程

例如输入224x224x3（rgb三通道），输出是32位深度，卷积核尺寸为5x5。

那么我们需要32个卷积核，每一个的尺寸为5x5x3（最后的3就是原图的rgb位深3），每一个卷积核的每一层是5x5（共3层）分别与原图的每层224x224卷积，然后将得到的三张新图叠加（算术求和），变成一张新的feature map。 每一个卷积核都这样操作，就可以得到32张新的feature map了。 也就是说：

不管输入图像的深度为多少，经过一个卷积核（filter），最后都通过下面的公式变成一个深度为1的特征图。不同的filter可以卷积得到不同的特征，也就是得到不同的feature map。。。

![img](https://img-blog.csdn.net/20171113170620654)

![img](https://img-blog.csdn.net/20171113170636779)

## filter的理解

filter有两种理解： 

在有的文档中，一个filter等同于一个卷积核：只是指定了卷积核的长宽深；

而有的情况（例如tensorflow等框架中，filter参数通常指定了卷积核的长、宽、深、个数四个参数），filter包含了卷积核形状和卷积核数量的概念：即filter既指定了卷积核的长宽深，也指定了卷积核的数量。

##  

## 理解tensorflow等框架中的参数 channel（feather map、卷积核数量）

在深度学习的算法学习中，都会提到 `channels` 这个概念。在一般的深度学习框架的 `conv2d` 中，如 [tensorflow](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) 、[mxnet](http://mxnet.incubator.apache.org/api/python/gluon/nn.html#mxnet.gluon.nn.Conv2D)，`channels` 都是必填的一个参数。

`channels` 该如何理解？先看一看不同框架中的解释文档。

首先，是 [tensorflow](https://www.tensorflow.org/tutorials/layers) 中给出的，对于输入样本中 `channels` 的含义。一般的RGB图片，`channels` 数量是 3 （红、绿、蓝）；而monochrome图片，`channels` 数量是 1 。

> **channels** : Number of color channels in the example images. For color images, the number of channels is 3 (red, green, blue). For monochrome images, there is just 1 channel (black). ——[tensorflow](https://www.tensorflow.org/tutorials/layers)

其次，[mxnet](http://mxnet.incubator.apache.org/api/python/gluon/nn.html#mxnet.gluon.nn.Conv2D) 中提到的，一般 `channels` 的含义是，**每个卷积层中卷积核的数量**。

> **channels** (int) : The dimensionality of the output space, i.e. the number of output channels (filters) in the convolution. ——[mxnet](http://mxnet.incubator.apache.org/api/python/gluon/nn.html#mxnet.gluon.nn.Conv2D)

为了更直观的理解，下面举个例子，图片使用自 [吴恩达老师的深度学习课程](http://mooc.study.163.com/learn/2001281004?tid=2001392030#/learn/content?type=detail&id=2001728687) 。

如下图，假设现有一个为 6×6×36×6×3 的图片样本，使用 3×3×33×3×3 的卷积核（filter）进行卷积操作。此时输入图片的 `channels` 为 33 ，而**卷积核中**的 `in_channels` 与 需要进行卷积操作的数据的 `channels` 一致（这里就是图片样本，为3）。

![cnn](https://img-blog.csdn.net/20180404135638186?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3NjY19sZWFybmluZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

接下来，进行卷积操作，卷积核中的27个数字与分别与样本对应相乘后，再进行求和，得到第一个结果。依次进行，最终得到 4×4 的结果。

![单个卷积核](https://img-blog.csdn.net/20180404113714719?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3NjY19sZWFybmluZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

上面步骤完成后，由于只有一个卷积核，所以最终得到的结果为 4×4×1 ， `out_channels` 为 1 。

在实际应用中，都会使用多个卷积核。这里如果再加一个卷积核，就会得到 4×4×2 的结果。

![多个卷积核](https://img-blog.csdn.net/20180404150134375?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3NjY19sZWFybmluZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

总结一下，我偏好把上面提到的 `channels` 分为三种：

1. 最初输入的图片样本的 `channels` ，取决于图片类型，比如RGB；
2. 卷积核中的 `in_channels` ，就是要操作的图像数据的feather  map张数，也就是卷积核的深度。（刚刚2中已经说了，就是上一次卷积的 `out_channels` ，如果是第一次做卷积，就是1中样本图片的 `channels）` ；
3. 卷积操作完成后输出的 ***\*`out_channels`\**** ，取决于**卷积核的数量（下层将产生的feather map数量）**。此时的 `out_channels` 也会作为下一次卷积时的卷积核的 `in_channels。`

说到这里，相信已经把 `channels` 讲的很清楚了。在CNN中，想搞清楚每一层的传递关系，主要就是 `height`,`width` 的变化情况，和 `channels` 的变化情况。

最后再看看 [tensorflow](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) 中 `tf.nn.conv2d` 的 `input` 和 `filter` 这两个参数。 
`input : [batch, in_height, in_width, in_channels]` ， 
`filter : [filter_height, filter_width, in_channels, **out_channels（卷积核的数量/下层将产生的feather map数量）**]` 。

里面的含义是不是很清楚了？

![conv2d](https://img-blog.csdn.net/20180404144950008?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3NjY19sZWFybmluZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

 

 

# CNN的学习过程：更新卷积核的值（更新提取的图像特征）

因为卷积核实际上就是如3x3，5x5这样子的权值（weights）矩阵。我们的网络要学习的，或者说要确定下来的，就是这些权值（weights）的数值。网络不断前后向的计算学习，一直在更新出合适的weights，也就是一直在更新卷积核们。卷积核在更新了，学习到的特征也就被更新了（因为卷积核的值（weights）变了，与上一层的map卷积计算的结果也就变了，得到的新map就也变了。）。对分类问题而言，目的就是：对图像提取特征，再以合适的特征来判断它所属的类别。类似这种概念：你有哪些个子的特征，我就根据这些特征，把你划分到某个类别去。

这样就很说的通了，卷积神经网络的一整套流程就是：更新卷积核参数（weights），就相当于是一直在更新所提取到的图像特征，以得到可以把图像正确分类的最合适的特征们。（一句话：更新weights以得到可以把图像正确分类的特征。） 































# RGB彩色图像的卷积过程（gif动图演示）

在CNN中，滤波器filter（带着一组固定权重的神经元）对局部输入数据进行卷积计算。每计算完一个数据窗口内的局部数据后，数据窗口不断平移滑动，直到计算完所有数据。这个过程中，有这么几个参数： 
　　a. 深度depth：神经元个数，决定输出的depth厚度。同时代表滤波器个数。
　　b. 步长stride：决定滑动多少步可以到边缘。

　　c. 填充值zero-padding：在外围边缘补充若干圈0，方便从初始位置以步长为单位可以刚好滑倒末尾位置，通俗地讲就是为了总长能被步长整除。 

> > > 　　![这里写图片描述](https://img-blog.csdn.net/20160705162205761) 

  cs231n课程中有一张卷积动图，貌似是用d3js 和一个util 画的，我**根据cs231n的卷积动图依次截取了18张图，然后用一gif 制图工具制作了一gif 动态卷积图。如下gif 图所示**

> ![img](https://img-blog.csdn.net/20160707204048899)

  可以看到：

- 两个神经元，即depth=2，意味着有两个滤波器。
- 数据窗口每次移动两个步长取3*3的局部数据，即stride=2。
- zero-padding=1。

  然后分别以两个滤波器filter为轴滑动数组进行卷积计算，得到两组不同的结果。

  如果初看上图，可能不一定能立马理解啥意思，但结合上文的内容后，理解这个动图已经不是很困难的事情：

- 左边是输入（**7\*7\*3**中，7*7代表图像的像素/长宽，3代表R、G、B 三个颜色通道）
- 中间部分是两个不同的滤波器Filter w0、Filter w1
- 最右边则是两个不同的输出

  随着左边数据窗口的平移滑动，滤波器Filter w0 / Filter w1对不同的局部数据进行卷积计算。

  值得一提的是：

1. 左边数据在变化，每次滤波器都是针对某一局部的数据窗口进行卷积，这就是所谓的CNN中的**局部感知**机制。

- 打个比方，滤波器就像一双眼睛，人类视角有限，一眼望去，只能看到这世界的局部。如果一眼就看到全世界，你会累死，而且一下子接受全世界所有信息，你大脑接收不过来。当然，即便是看局部，针对局部里的信息人类双眼也是有偏重、偏好的。比如看美女，对脸、胸、腿是重点关注，所以这3个输入的权重相对较大。

与此同时，数据窗口滑动，导致输入在变化，但中间滤波器Filter w0的权重（即每个神经元连接数据窗口的权重）是固定不变的，这个权重不变即所谓的CNN中的**参数（权重）共享**机制。

- 再打个比方，某人环游全世界，所看到的信息在变，但采集信息的双眼不变。btw，不同人的双眼 看同一个局部信息 所感受到的不同，即一千个读者有一千个哈姆雷特，所以不同的滤波器 就像不同的双眼，不同的人有着不同的反馈结果。

  我第一次看到上面这个动态图的时候，只觉得很炫，另外就是据说计算过程是“相乘后相加”，但到底具体是个怎么相乘后相加的计算过程 则无法一眼看出，网上也没有一目了然的计算过程。本文来细究下。

  首先，我们来分解下上述动图，如下图

> ![img](https://img-blog.csdn.net/20160707204919497)

  接着，我们细究下上图的具体计算过程。即上图中的输出结果1具体是怎么计算得到的呢？其实，类似wx + b，w对应滤波器Filter w0，x对应不同的数据窗口，b对应Bias b0，相当于滤波器Filter w0与一个个数据窗口相乘再求和后，最后加上Bias b0得到输出结果1，如下过程所示：

> ![img](https://img-blog.csdn.net/20160707205622297) ![img](https://img-blog.csdn.net/20160707205640719)

1* 0 + 1*0 + -1*0 

+

-1*0 + 0*0 + **1\*1**

+

-1*0 + -1*0 + 0*1 

+

![img](https://img-blog.csdn.net/20160707205653969)![img](https://img-blog.csdn.net/20160707205705937)

-1*0 + 0*0 + -1*0

+

0*0 + 0*1 + **-1\*1**

+

1*0 + -1*0 + 0*2

+

![img](https://img-blog.csdn.net/20160707205720140)![img](https://img-blog.csdn.net/20160707205732156)

0*0 + 1*0 + 0*0

+

1*0 + 0*2 + 1*0

+

0*0 + -1*0 + 1*0

+ 

**1**

=

1

  然后滤波器Filter w0固定不变，数据窗口向右移动2步，继续做内积计算，得到0的输出结果

> ![img](https://img-blog.csdn.net/20160707230038086)

  最后，换做另外一个不同的滤波器Filter w1、不同的偏置Bias b1，再跟图中最左边的数据窗口做卷积，可得到另外一个不同的输出。

> ![img](https://img-blog.csdn.net/20160707230115204)



















